{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Skills Network Logo\n",
    "\n",
    "Space X Falcon 9 First Stage Landing Prediction\n",
    "Assignment: Machine Learning Prediction\n",
    "Estimated time needed: 60 minutes\n",
    "\n",
    "Space X advertises Falcon 9 rocket launches on its website with a cost of 62 million dollars; other providers cost upward of 165 million dollars each, much of the savings is because Space X can reuse the first stage. Therefore if we can determine if the first stage will land, we can determine the cost of a launch. This information can be used if an alternate company wants to bid against space X for a rocket launch. In this lab, you will create a machine learning pipeline to predict if the first stage will land given the data from the preceding labs.\n",
    "\n",
    "\n",
    "\n",
    "Several examples of an unsuccessful landing are shown here:\n",
    "\n",
    "\n",
    "\n",
    "Most unsuccessful landings are planed. Space X; performs a controlled landing in the oceans.\n",
    "\n",
    "Objectives\n",
    "Perform exploratory Data Analysis and determine Training Labels\n",
    "\n",
    "create a column for the class\n",
    "Standardize the data\n",
    "Split into training data and test data\n",
    "-Find best Hyperparameter for SVM, Classification Trees and Logistic Regression\n",
    "\n",
    "Find the method performs best using test data\n",
    "Import Libraries and Define Auxiliary Functions\n",
    "import piplite\n",
    "await piplite.install(['numpy'])\n",
    "await piplite.install(['pandas'])\n",
    "await piplite.install(['seaborn'])\n",
    "We will import the following libraries for the lab\n",
    "\n",
    "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
    "import pandas as pd\n",
    "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "import numpy as np\n",
    "# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\n",
    "import matplotlib.pyplot as plt\n",
    "#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n",
    "import seaborn as sns\n",
    "# Preprocessing allows us to standarsize our data\n",
    "from sklearn import preprocessing\n",
    "# Allows us to split our data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Allows us to test parameters of classification algorithms and find the best one\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Logistic Regression classification algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Support Vector Machine classification algorithm\n",
    "from sklearn.svm import SVC\n",
    "# Decision Tree classification algorithm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# K Nearest Neighbors classification algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "This function is to plot the confusion matrix.\n",
    "\n",
    "def plot_confusion_matrix(y,y_predict):\n",
    "    \"this function plots the confusion matrix\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(y, y_predict)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['did not land', 'land']); ax.yaxis.set_ticklabels(['did not land', 'landed']) \n",
    "    plt.show() \n",
    "Load the dataframe\n",
    "Load the data\n",
    "\n",
    "from js import fetch\n",
    "import io\n",
    "\n",
    "URL1 = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_2.csv\"\n",
    "resp1 = await fetch(URL1)\n",
    "text1 = io.BytesIO((await resp1.arrayBuffer()).to_py())\n",
    "data = pd.read_csv(text1)\n",
    "data.head()\n",
    "FlightNumber\tDate\tBoosterVersion\tPayloadMass\tOrbit\tLaunchSite\tOutcome\tFlights\tGridFins\tReused\tLegs\tLandingPad\tBlock\tReusedCount\tSerial\tLongitude\tLatitude\tClass\n",
    "0\t1\t2010-06-04\tFalcon 9\t6104.959412\tLEO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0003\t-80.577366\t28.561857\t0\n",
    "1\t2\t2012-05-22\tFalcon 9\t525.000000\tLEO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0005\t-80.577366\t28.561857\t0\n",
    "2\t3\t2013-03-01\tFalcon 9\t677.000000\tISS\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0007\t-80.577366\t28.561857\t0\n",
    "3\t4\t2013-09-29\tFalcon 9\t500.000000\tPO\tVAFB SLC 4E\tFalse Ocean\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB1003\t-120.610829\t34.632093\t0\n",
    "4\t5\t2013-12-03\tFalcon 9\t3170.000000\tGTO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB1004\t-80.577366\t28.561857\t0\n",
    "URL2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_3.csv'\n",
    "resp2 = await fetch(URL2)\n",
    "text2 = io.BytesIO((await resp2.arrayBuffer()).to_py())\n",
    "X = pd.read_csv(text2)\n",
    "X.head(100)\n",
    "FlightNumber\tPayloadMass\tFlights\tBlock\tReusedCount\tOrbit_ES-L1\tOrbit_GEO\tOrbit_GTO\tOrbit_HEO\tOrbit_ISS\t...\tSerial_B1058\tSerial_B1059\tSerial_B1060\tSerial_B1062\tGridFins_False\tGridFins_True\tReused_False\tReused_True\tLegs_False\tLegs_True\n",
    "0\t1.0\t6104.959412\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "1\t2.0\t525.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "2\t3.0\t677.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "3\t4.0\t500.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "4\t5.0\t3170.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "85\t86.0\t15400.000000\t2.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
    "86\t87.0\t15400.000000\t3.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
    "87\t88.0\t15400.000000\t6.0\t5.0\t5.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
    "88\t89.0\t15400.000000\t3.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
    "89\t90.0\t3681.000000\t1.0\t5.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t1.0\t0.0\t0.0\t1.0\n",
    "90 rows Ã— 83 columns\n",
    "\n",
    "TASK 1\n",
    "Create a NumPy array from the column Class in data, by applying the method to_numpy() then assign it to the variable Y,make sure the output is a Pandas series (only one bracket df['name of column']).\n",
    "\n",
    "Y=data[\"Class\"].to_numpy()\n",
    "TASK 2\n",
    "Standardize the data in X then reassign it to the variable X using the transform provided below.\n",
    "\n",
    "# students get this \n",
    "transform = preprocessing.StandardScaler()\n",
    "X = transform.fit(X).transform(X)\n",
    "We split the data into training and testing data using the function train_test_split. The training data is divided into validation data, a second set used for training data; then the models are trained and hyperparameters are selected using the function GridSearchCV.\n",
    "\n",
    "TASK 3\n",
    "Use the function train_test_split to split the data X and Y into training and test data. Set the parameter test_size to 0.2 and random_state to 2. The training data and test data should be assigned to the following labels.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
    "we can see we only have 18 test samples.\n",
    "\n",
    "Y_test.shape\n",
    "(18,)\n",
    "TASK 4\n",
    "Create a logistic regression object then create a GridSearchCV object logreg_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
    "\n",
    "parameters ={'C':[0.01,0.1,1],\n",
    "             'penalty':['l2'],\n",
    "             'solver':['lbfgs']}\n",
    "parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}# l1 lasso l2 ridge\n",
    "lr=LogisticRegression()\n",
    "logreg_cv = GridSearchCV(estimator=lr, cv=10, param_grid=parameters).fit(X_train, Y_train)\n",
    "We output the GridSearchCV object for logistic regression. We display the best parameters using the data attribute best_params_ and the accuracy on the validation data using the data attribute best_score_.\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)\n",
    "tuned hpyerparameters :(best parameters)  {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "accuracy : 0.8464285714285713\n",
    "TASK 5\n",
    "Calculate the accuracy on the test data using the method score:\n",
    "\n",
    "logreg_score = logreg_cv.score(X_test, Y_test)\n",
    "print(\"score :\", logreg_score)\n",
    "score : 0.8333333333333334\n",
    "Lets look at the confusion matrix:\n",
    "\n",
    "yhat=logreg_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,yhat)\n",
    "\n",
    "Examining the confusion matrix, we see that logistic regression can distinguish between the different classes. We see that the major problem is false positives.\n",
    "\n",
    "TASK 6\n",
    "Create a support vector machine object then create a GridSearchCV object svm_cv with cv - 10. Fit the object to find the best parameters from the dictionary parameters.\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf','poly','rbf', 'sigmoid'),\n",
    "              'C': np.logspace(-3, 3, 5),\n",
    "              'gamma':np.logspace(-3, 3, 5)}\n",
    "svm = SVC()\n",
    "svm_cv = GridSearchCV(estimator=svm, cv=10, param_grid=parameters).fit(X_train, Y_train)\n",
    " \n",
    "print(\"tuned hpyerparameters :(best parameters) \",svm_cv.best_params_)\n",
    "print(\"accuracy :\",svm_cv.best_score_)\n",
    "tuned hpyerparameters :(best parameters)  {'C': 1.0, 'gamma': 0.03162277660168379, 'kernel': 'sigmoid'}\n",
    "accuracy : 0.8482142857142856\n",
    "TASK 7\n",
    "Calculate the accuracy on the test data using the method score:\n",
    "\n",
    "svm_cv_score = svm_cv.score(X_test, Y_test)\n",
    "print(\"score :\",svm_cv_score)\n",
    "score : 0.8333333333333334\n",
    "We can plot the confusion matrix\n",
    "\n",
    "yhat=svm_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,yhat)\n",
    "\n",
    "TASK 8\n",
    "Create a decision tree classifier object then create a GridSearchCV object tree_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
    "\n",
    "parameters = {'criterion': ['gini', 'entropy'],\n",
    "     'splitter': ['best', 'random'],\n",
    "     'max_depth': [2*n for n in range(1,10)],\n",
    "     'max_features': ['sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_cv = GridSearchCV(estimator=tree, cv=10, param_grid=parameters,).fit(X_train, Y_train)\n",
    " \n",
    "print(\"tuned hpyerparameters :(best parameters) \",tree_cv.best_params_)\n",
    "print(\"accuracy :\",tree_cv.best_score_)\n",
    "tuned hpyerparameters :(best parameters)  {'criterion': 'gini', 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "accuracy : 0.8875000000000002\n",
    "TASK 9\n",
    "Calculate the accuracy of tree_cv on the test data using the method score:\n",
    "\n",
    "tree_cv_score = svm_cv.score(X_test, Y_test)\n",
    "print(\"score :\",tree_cv_score)\n",
    "score : 0.8333333333333334\n",
    "We can plot the confusion matrix\n",
    "\n",
    "yhat = tree_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,yhat)\n",
    "\n",
    "TASK 10\n",
    "Create a k nearest neighbors object then create a GridSearchCV object knn_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
    "\n",
    "parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "              'p': [1,2]}\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(estimator=KNN, cv=10, param_grid=parameters).fit(X_train, Y_train)\n",
    " \n",
    "print(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\n",
    "print(\"accuracy :\",knn_cv.best_score_)\n",
    "tuned hpyerparameters :(best parameters)  {'algorithm': 'auto', 'n_neighbors': 10, 'p': 1}\n",
    "accuracy : 0.8482142857142858\n",
    "TASK 11\n",
    "Calculate the accuracy of knn_cv on the test data using the method score:\n",
    "\n",
    "knn_cv_score = knn_cv.score(X_test, Y_test)\n",
    "print(\"score :\",knn_cv_score)\n",
    "score : 0.8333333333333334\n",
    "We can plot the confusion matrix\n",
    "\n",
    "yhat = knn_cv.predict(X_test)\n",
    "plot_confusion_matrix(Y_test,yhat)\n",
    "\n",
    "TASK 12\n",
    "Find the method performs best:\n",
    "\n",
    "accuracy = [svm_cv_score, logreg_score, knn_cv_score, tree_cv_score]\n",
    "accuracy = [i * 100 for i in accuracy]\n",
    "\n",
    "method = ['Support Vector Machine', 'Logistic Regression', 'K Nearest Neighbour', 'Decision Tree']\n",
    "models = {'ML Method':method, 'Accuracy Score (%)':accuracy}\n",
    "\n",
    "ML_df = pd.DataFrame(models)\n",
    "ML_df\n",
    "ML Method\tAccuracy Score (%)\n",
    "0\tSupport Vector Machine\t83.333333\n",
    "1\tLogistic Regression\t83.333333\n",
    "2\tK Nearest Neighbour\t83.333333\n",
    "3\tDecision Tree\t83.333333\n",
    "models = {'KNeighbors':knn_cv.best_score_,\n",
    "              'DecisionTree':tree_cv.best_score_,\n",
    "              'LogisticRegression':logreg_cv.best_score_,\n",
    "              'SupportVector': svm_cv.best_score_}\n",
    "\n",
    "bestalgorithm = max(models, key=models.get)\n",
    "print('Best model is', bestalgorithm,'with a score of', models[bestalgorithm])\n",
    "if bestalgorithm == 'DecisionTree':\n",
    "    print('Best params is :', tree_cv.best_params_)\n",
    "if bestalgorithm == 'KNeighbors':\n",
    "    print('Best params is :', knn_cv.best_params_)\n",
    "if bestalgorithm == 'LogisticRegression':\n",
    "    print('Best params is :', logreg_cv.best_params_)\n",
    "if bestalgorithm == 'SupportVector':\n",
    "    print('Best params is :', svm_cv.best_params_)\n",
    "Best model is DecisionTree with a score of 0.8875000000000002\n",
    "Best params is : {'criterion': 'gini', 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "Authors\n",
    "Pratiksha Verma\n",
    "\n",
    "Change Log\n",
    "Date (YYYY-MM-DD)\tVersion\tChanged By\tChange Description\n",
    "2022-11-09\t1.0\tPratiksha Verma\tConverted initial version to Jupyterlite\n",
    "IBM Corporation 2022. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
